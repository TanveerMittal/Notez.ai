{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desirable-contractor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from gensim import corpora, models\n",
    "from gensim.utils import simple_preprocess\n",
    "from collections import Counter, defaultdict\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demonstrated-brazil",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../src/backend\")\n",
    "from question_generation.pipelines import pipeline as qg_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "young-container",
   "metadata": {},
   "outputs": [],
   "source": [
    "qg = qg_pipeline(\"question-generation\", use_cuda=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "toxic-vision",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_coherent_qa(qa):\n",
    "    if len(qa) == 1:\n",
    "        return qa[0]\n",
    "    scores = [score(pair[\"question\"] + \" \" + pair[\"answer\"]) for pair in qa]\n",
    "    idx = np.argmax(scores)\n",
    "    return qa[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "several-dialogue",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from pytorch_pretrained_bert import OpenAIGPTTokenizer, OpenAIGPTModel, OpenAIGPTLMHeadModel\n",
    "# Load pre-trained model (weights)\n",
    "model = OpenAIGPTLMHeadModel.from_pretrained('openai-gpt')\n",
    "model.eval()\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = OpenAIGPTTokenizer.from_pretrained('openai-gpt')\n",
    "def score(sentence):\n",
    "    tokenize_input = tokenizer.tokenize(sentence)\n",
    "    tensor_input = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_input)])\n",
    "    loss=model(tensor_input, lm_labels=tensor_input)\n",
    "    return math.exp(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joint-membership",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../..\")\n",
    "with open(\"data/econ3_8.txt\") as f:\n",
    "    doc1 = f.read().strip()\n",
    "#words = doc.split(' ')\n",
    "#processed_doc = preprocess(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patent-offering",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/econ3_7.txt\") as f:\n",
    "    doc2 = f.read().strip()\n",
    "#words = doc.split(' ')\n",
    "#processed_doc = preprocess(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passing-florence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO filter meaningless sentences\n",
    "def get_teacher_text(doc):\n",
    "    \"Extracts text spoken by teacher\"\n",
    "    matches = re.findall(r\"(?:(?:[A-Z][a-z]*\\s)*(?:[A-Z][a-z]*)):\", doc)\n",
    "    splits = re.split(r\"(?:(?:[A-Z][a-z]*\\s)*(?:[A-Z][a-z]*)):\", doc)[1:]\n",
    "    teacher = Counter(matches).most_common(1)[0][0]\n",
    "    return ''.join([splits[i] for i in range(len(splits)) if matches[i] == teacher])\n",
    "\n",
    "def clean(doc):\n",
    "    return re.sub(r\"[',]\", \"\", re.sub(r\"\\s+\", \" \", doc))\n",
    "\n",
    "def get_sents(doc):\n",
    "    return [sent for sent in sent_tokenize(doc)]\n",
    "def get_filtered_sents(doc):\n",
    "    return [sent for sent in sent_tokenize(doc) if len(re.findall(r\"\\s\", sent)) > 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polished-aquarium",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = get_sents(clean(get_teacher_text(doc1)))\n",
    "filtered_sents = get_filtered_sents(clean(get_teacher_text(doc1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "included-eight",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "#stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        # deacc=True removes punctuations\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) \n",
    "             if word not in stop_words] for doc in texts]\n",
    "\n",
    "data_words = list(sent_to_words(filtered_sents))\n",
    "# remove stop words\n",
    "data_words = remove_stopwords(data_words)\n",
    "\n",
    "id2word = corpora.Dictionary(data_words)\n",
    "# Create Corpus\n",
    "texts = data_words\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "num_topics = 5\n",
    "\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=num_topics)\n",
    "lda_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heated-practice",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = defaultdict(str)\n",
    "labels = [np.array(lda_model.get_document_topics(sent))[:, 1].argmax() for sent in corpus]\n",
    "confidences = [np.array(lda_model.get_document_topics(sent))[labels[i],1] for i, sent in enumerate(corpus)]\n",
    "for i in range(len(filtered_sents)):\n",
    "    clusters[labels[i]] += filtered_sents[i] + \" \"\n",
    "for topic in clusters:\n",
    "    clusters[topic] = re.sub(r\"\\s+\", \" \", clusters[topic])\n",
    "    clusters[topic] = re.sub(r\"[',]\", \"\", clusters[topic])\n",
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contrary-cable",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame().assign(topic=labels, confidence=confidences, sentences=filtered_sents)\n",
    "top_sentences = df.groupby(\"topic\").apply(lambda grp: df.loc[grp['confidence'].nlargest(5).index])\n",
    "top_sentences = top_sentences.assign(questions=top_sentences[\"sentences\"].apply(lambda sent: get_most_coherent_qa(qg(sent))))\n",
    "top_sentences.index = top_sentences.index.droplevel(0)\n",
    "top_sentences = top_sentences.reset_index(drop=True)\n",
    "top_sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrative-tooth",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "featured-meeting",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "questions = {}\n",
    "for i in range(num_topics):\n",
    "    questions[i] = {\"questions\":top_sentences[top_sentences[\"topic\"] == i][\"questions\"].to_list()}\n",
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "better-typing",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from src.backend.nlp import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serious-jacksonville",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = pipeline(\"summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proof-universal",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_sentences = df.groupby(\"topic\").apply(lambda grp: df.loc[grp['confidence'].nlargest(10).index])\n",
    "summary_sentences.index = summary_sentences.index.droplevel(0)\n",
    "summary_sentences = summary_sentences.reset_index(drop=True)\n",
    "to_summarize = summary_sentences.groupby(\"topic\")[\"sentences\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tutorial-court",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = to_summarize.apply(lambda x: ' '.join([t[\"summary_text\"] for t in summarizer(x)])) \n",
    "summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cheap-shame",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"topic\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "public-oxford",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
