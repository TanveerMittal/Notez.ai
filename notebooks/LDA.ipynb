{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "broadband-intermediate",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tanve\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\tanve\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from gensim import corpora, models\n",
    "from gensim.utils import simple_preprocess\n",
    "from collections import Counter, defaultdict\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "demonstrated-brazil",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: '../src/backend'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-b7a6175dccdf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"../src/backend\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mquestion_generation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpipelines\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpipeline\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mqg_pipeline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: '../src/backend'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"../src/backend\")\n",
    "from question_generation.pipelines import pipeline as qg_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "young-container",
   "metadata": {},
   "outputs": [],
   "source": [
    "qg = qg_pipeline(\"question-generation\", use_cuda=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "toxic-vision",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_coherent_qa(qa):\n",
    "    if len(qa) == 1:\n",
    "        return qa[0]\n",
    "    scores = [score(pair[\"question\"] + \" \" + pair[\"answer\"]) for pair in qa]\n",
    "    idx = np.argmax(scores)\n",
    "    return qa[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "several-dialogue",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "from pytorch_pretrained_bert import OpenAIGPTTokenizer, OpenAIGPTModel, OpenAIGPTLMHeadModel\n",
    "# Load pre-trained model (weights)\n",
    "model = OpenAIGPTLMHeadModel.from_pretrained('openai-gpt')\n",
    "model.eval()\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = OpenAIGPTTokenizer.from_pretrained('openai-gpt')\n",
    "def score(sentence):\n",
    "    tokenize_input = tokenizer.tokenize(sentence)\n",
    "    tensor_input = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_input)])\n",
    "    loss=model(tensor_input, lm_labels=tensor_input)\n",
    "    return math.exp(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "joint-membership",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../..\")\n",
    "with open(\"data/econ3_8.txt\") as f:\n",
    "    doc1 = f.read().strip()\n",
    "#words = doc.split(' ')\n",
    "#processed_doc = preprocess(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "patent-offering",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/econ3_7.txt\") as f:\n",
    "    doc2 = f.read().strip()\n",
    "#words = doc.split(' ')\n",
    "#processed_doc = preprocess(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "passing-florence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO filter meaningless sentences\n",
    "def get_teacher_text(doc):\n",
    "    \"Extracts text spoken by teacher\"\n",
    "    matches = re.findall(r\"(?:(?:[A-Z][a-z]*\\s)*(?:[A-Z][a-z]*)):\", doc)\n",
    "    splits = re.split(r\"(?:(?:[A-Z][a-z]*\\s)*(?:[A-Z][a-z]*)):\", doc)[1:]\n",
    "    teacher = Counter(matches).most_common(1)[0][0]\n",
    "    return ''.join([splits[i] for i in range(len(splits)) if matches[i] == teacher])\n",
    "\n",
    "def clean(doc):\n",
    "    return re.sub(r\"[',]\", \"\", re.sub(r\"\\s+\", \" \", doc))\n",
    "\n",
    "def get_sents(doc):\n",
    "    return [sent for sent in sent_tokenize(doc)]\n",
    "def get_filtered_sents(doc):\n",
    "    return [sent for sent in sent_tokenize(doc) if len(re.findall(r\"\\s\", sent)) > 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "polished-aquarium",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = get_sents(clean(get_teacher_text(doc1)))\n",
    "filtered_sents = get_filtered_sents(clean(get_teacher_text(doc1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "included-eight",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-21 04:55:01.955 INFO    gensim.corpora.dictionary: adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-02-21 04:55:01.974 INFO    gensim.corpora.dictionary: built Dictionary(729 unique tokens: ['behind', 'feel', 'hiding', 'im', 'like']...) from 200 documents (total 2201 corpus positions)\n",
      "2021-02-21 04:55:01.990 INFO    gensim.models.ldamodel: using symmetric alpha at 0.2\n",
      "2021-02-21 04:55:01.991 INFO    gensim.models.ldamodel: using symmetric eta at 0.2\n",
      "2021-02-21 04:55:01.992 INFO    gensim.models.ldamodel: using serial LDA version on this node\n",
      "2021-02-21 04:55:01.997 INFO    gensim.models.ldamulticore: running online LDA training, 5 topics, 1 passes over the supplied corpus of 200 documents, updating every 6000 documents, evaluating every ~200 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2021-02-21 04:55:01.997 WARNING gensim.models.ldamulticore: too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "2021-02-21 04:55:02.013 INFO    gensim.models.ldamulticore: training LDA model using 3 processes\n",
      "2021-02-21 04:55:02.072 INFO    gensim.models.ldamulticore: PROGRESS: pass 0, dispatched chunk #0 = documents up to #200/200, outstanding queue size 1\n",
      "2021-02-21 04:55:03.814 INFO    gensim.models.ldamodel: topic #0 (0.200): 0.029*\"growth\" + 0.011*\"chapter\" + 0.010*\"average\" + 0.010*\"rate\" + 0.009*\"things\" + 0.009*\"much\" + 0.009*\"know\" + 0.008*\"rates\" + 0.008*\"really\" + 0.008*\"get\"\n",
      "2021-02-21 04:55:03.815 INFO    gensim.models.ldamodel: topic #1 (0.200): 0.021*\"countries\" + 0.018*\"per\" + 0.018*\"know\" + 0.016*\"gdp\" + 0.016*\"capita\" + 0.014*\"growth\" + 0.012*\"going\" + 0.010*\"okay\" + 0.010*\"im\" + 0.010*\"time\"\n",
      "2021-02-21 04:55:03.816 INFO    gensim.models.ldamodel: topic #2 (0.200): 0.021*\"one\" + 0.016*\"going\" + 0.015*\"per\" + 0.015*\"average\" + 0.015*\"thats\" + 0.014*\"right\" + 0.013*\"growth\" + 0.012*\"year\" + 0.012*\"gdp\" + 0.012*\"years\"\n",
      "2021-02-21 04:55:03.817 INFO    gensim.models.ldamodel: topic #3 (0.200): 0.022*\"one\" + 0.019*\"okay\" + 0.016*\"average\" + 0.016*\"growth\" + 0.015*\"going\" + 0.014*\"would\" + 0.013*\"thats\" + 0.012*\"years\" + 0.010*\"china\" + 0.010*\"year\"\n",
      "2021-02-21 04:55:03.819 INFO    gensim.models.ldamodel: topic #4 (0.200): 0.042*\"growth\" + 0.024*\"rate\" + 0.017*\"gdp\" + 0.016*\"per\" + 0.015*\"year\" + 0.013*\"one\" + 0.013*\"know\" + 0.012*\"okay\" + 0.009*\"real\" + 0.009*\"living\"\n",
      "2021-02-21 04:55:03.819 INFO    gensim.models.ldamodel: topic diff=3.011544, rho=1.000000\n",
      "2021-02-21 04:55:03.904 INFO    gensim.models.ldamodel: -6.736 per-word bound, 106.6 perplexity estimate based on a held-out corpus of 200 documents with 2201 words\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<gensim.models.ldamulticore.LdaMulticore at 0x23a98ff2e48>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "#stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        # deacc=True removes punctuations\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) \n",
    "             if word not in stop_words] for doc in texts]\n",
    "\n",
    "data_words = list(sent_to_words(filtered_sents))\n",
    "# remove stop words\n",
    "data_words = remove_stopwords(data_words)\n",
    "\n",
    "id2word = corpora.Dictionary(data_words)\n",
    "# Create Corpus\n",
    "texts = data_words\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "num_topics = 5\n",
    "\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=num_topics)\n",
    "lda_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "heated-practice",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(str,\n",
       "            {2: 'Looks like I feel Im only hiding my tiredness behind my makeup. So two students who were in a dorm on campus had to evacuate because of a fire alarm. Ill situate one during an online exam because theres so much drama. I think pertaining to this but hopefully you figured out you didnt need to know that for the midterm. So these are just some useful tools that will start with and then well start the formal chapter 19 right after this. So in this case we would take 200 times 1.04 squared to get that same number right now that second way of doing it is useful because we dont have to go step by step through every year. In fact We can say in general if we want to know what output is in your team. Okay so its a really useful formula for saying Okay if they have 4% growth and if they start out at this certain amount. Because a lot of times this is really useful to be able to answer this question. Lets do the next algebraic step were going to take one plus a To the Equals 3207 Over 1000. Minus one you would get the answer of a 220.7% increase over 20 years so thats thats accurate. If average annual growth were really 11% per year output would have been 8000 rather than 3207. You can imagine a math savvy politician claiming that during their administration average annual growth was much higher than it would be when you if you calculated it the compound way. And lets suppose that variable a is simply the product A B C and D. So its a is equal to be time seat time Steve writes the product. Okay so I had my pull up who gave that really good answer was that week. All right so So you can see you know now off the top of your head. You can do it in the opposite way or in the way we did it with this question. And so thats why we do this calculation of average annual kind of thing. So in many ways a low income person in the US today. So for example her husband Prince Albert died of typhoid right when he was relatively young and she wore Black. Heres GDP per capita in some selected European economies from 1300 to 1800 is a three year average. Suppose we have two countries country a country be and they start with the same per capita GDP say $1000 Suppose it over the next 100 years on average country has 2% growth and country be has 4% growth. Okay lets calculate where each of those countries will end up at the end of 100 years. Remember theyre starting off at the same GDP per capita so country has 1000 And thats going to compound on average 2% per year. But if it keeps going on and on that lower average annual growth will affect your standard of living in significant ways going forward. 54321 Okay this is good because this is you have to do that exponential nation. They took 8032 divided by 1039 Were talking about 30 years here. But you can see the big difference at the end of those 30 years All right another really useful approximation. The rule of 72 and sometimes youll see it as the rule of 70. And thats approximately 72 divided by the annual percentage growth of the variable stated in percent terms. Alright so Suppose the GDP per capita of a country grows at 3% per year on average than GDP per capita will double in. Of GDP per capita in China was 7.1% we figured that out a couple slides ago. And thats why that rule of 72 is such a useful thing because In many governments you know a lot of people a lot of the politicians probably cant do a lot of exponential nation. A workers marginal product is the extra output produced by adding the worker to your company say or to the field. Now youve heard about the law of diminishing returns right he called one you know about diminishing returns because of the law of diminishing returns the marginal product of labor. That means you have a positive marginal product of labor right adding more workers we assume increases output. Okay so thats how you can remember that its concave All right. I mean depends on how far we get an 11 and then well move on to the other part. ',\n",
       "             4: 'I said oh if I didnt have that conference I need to go to later in Catholic I would have canceled today. Yeah so its probably it might have been less exhausting for you guys because Youre in the quiet of your own place instead of a big room. Okay so lets go ahead and get started and I hope that everybody I guess you guys survived the exam because youre here this morning so We we looked at some of the score. So examples of series exhibiting these are the GDP the price level stock prices population. In fact were going to use some approximations from exponential growth to talk about compound growth. Over a certain amount of time now actual growth will vary from year to year but its nice to have an average. So Im sure youve all seen compounding of the interest rates before. So lets let little A underline this OK this little A here in bold be the growth rate not in percent. So if the growth rate is 4% and you write it as a equals point 04 Okay so in year one. OK. OK. Now the reason we have to do this with exponent is that you would get quite in most cases a very wrong answer. So were looking at this problem here where we have 3207 verses 1000 so if you do the percent change. It turns out that if we care about the growth rate of a It is approximately equal to the sum of the growth rates of all the things in the product. So this one has a geometric growth rate of G. Some be this growth rate is G subsea. So if GDP grows by 6% per year and population grows by 2% per year. What is the growth rate of GDP per capita and remember that this means per person. All right because they are ratio we know the growth of GDP per capita is going to be approximately equal to the growth rate the numerator minus the growth rate the denominator. Okay second problem if real GDP grows 3% per year and the inflation rate and consider that to be using the deflator The GDP deflator remember Okay its 5% per year. What is the growth rate of nominal GDP and let me start that poll Is so weird how the pole will come up unless its hiding somewhere. Times the price level okay or Actually you dont even need to rewrite it so That means that The growth rate. If somebody tells you Oh inflation is this nominal GDP was that what was the real loves the growth rate a real gap. But one of the really amazing things is Japan that started so far behind ended up with GDP per capita. And we want to understand why it is that different countries have different growth experiences. What does that mean okay it means We know that real GDP doesnt perfectly capture standards of living but we know that theres a lot of correlation between real GDP per capita. But what looked like oh one had 2% growth one had 4% growth. So that was 7.1% if you do the same thing for India you can calculate this on your own to check the answer if you do the same thing for India you get a 4.4% So China was growing at amazing 7.1% on average each year. Why does this work you know theres a reason but i i dont recall offhand. And were going to talk about some of the details of that little bit later in this chapter with some of the stuff that Im going to supplement Now oh actually heres some current events. Now I think it is yeah from where they stood in 2010 including the now chief threshold of $10000 per capita annual income. While lifting living standards for remaining 5 million people still considered severely impoverished. So if everybody understands the rule of 72 then that makes it easier to talk about this. Okay first lets do a quick review of those econ one concepts. ',\n",
       "             0: 'You know Im going to think Im Im going to see if I can miss a second conference so that after the second mid term we dont have to Its so much more exhausting than an actual midterm at least for us is it for you guys. I didnt feel like anybody felt like they were required to come they hadnt gotten out of bed yet. So was this down to the number of different things that happened last night. It was near UC Santa Cruz and Luckily it came back up in time but It was great. Obviously the essay questions have been answered but the the Mean was not as high as usual we think its just because of all the stuff going on so that it no promises but that likely is likely to meet a Generous curb. Alright so we are starting chapter 19 And one of the first things were going to do is talk about compound growth because thats such an important part of thinking about how standards of living increase over time. Theres some parts in here that also apply to chapter 18 and usually I teach both in this part. For the first midterm because it applies to chapter 18 and 19 but we didnt get to it. So thats why it wasnt on the midterm and I accidentally left some discussion questions. Now closely related concepts are what are called geometric growth an exponential growth. The 200 and we have 4% growth than by your one were going to have 200 times one plus point 04 to get 208 Alright so thats how much you grew How about your to well we can just take the value that we found from your one we compound that meaning we multiply it by 1.04 And that gives us Twitter and 16.32 alright so its just you can see just doing it step by step by step but we suppose that I gave you the 200 and year zero and said Tell me what the value going to be for output in your in your two. Then we can take your t minus one times one plus A or we can go all the way back to whatever year we want to start this is year zero however many years distance And multiply that times one plus point oh four to the power team captain. Suppose I tell you that output why it which is the same thing is real GDP started out in year zero at 1020 years later it was 3207 What if I asked you what average annual compound growth rate must if it had experienced on average each year to be able to go from 1000 to 3207 There shouldnt have been a question like this on the midterm. So in this case youre starting with 1000 You know that theres some one plus this growth rate that were trying to solve for to the 20 because 20 years has gone by. And if youre using what you have programmable calculate you know so far youve been able to use that. And if you look at growth rates than its the sum and Ill show you that a little bit on an FYI mathematical thing but its really really useful to have this because you can just calculate things in your head. Now suppose instead a is equal to be divided by see so as the ratio of B to C. Will just as when things are multiplied the growth rates were the the some of the the growth rates of each of them when things are divided the growth rate of A is approximately equal to the growth rate of B minus the growth rate of C. So ratios. So the relationship that I said on the previous page was an approximation. And if you remember your exponential things These growth rates add in the exponent. So which therefore has a growth rate of the some of the growth rates. You dont have to You know sit down and work it out with paper and pen. So that is why this is equal to 3% plus 5% Equals 8% So again this gives you a tool to in your head. Thats why I have 10 its from 2008 to 2018 and we know that That CPI is going to end up at 1.83 Will take one plus A to the 10 equals 1.83 divided by 1.5 and one plus A equals 1.83 over 1.5 to the one over 10 and solve For a which is going to be 0.02 which implies 2% Okay average annual inflation. Because what we had done on fate given what you knew on the mid term we would say what was the inflation rate over this five year period or something you would just calculate the percent change over the whole period. That was that question questions before I close this and start the Chapter 19 Alright so What were trying to understand in this chapter is through remarkable rice and living standards. Okay so heres a table that compares 1872 2010 And this is a 1990 international dollars and you know as usual you dont have to remember that the actual numbers but just think of this as almost like an index of their real output comparing it across countries and across time. Some but not as much as China and Ghana group but not as much even as India. Black morning for something like 60 years because she lived forever you know And today it would be so easy to cure that even if you were relatively poor right society would would have the medicine to help cure you. So its a tremendous increase in living standards you know a poor person today in the US wouldnt be living in Buckingham Palace but they would have Access to so much else that people couldnt even dream of in the 1800s or they might have dreamed of it but knew that they didnt have it. Alright so There were long long periods of stagnation and by some measures. The citizens of the Roman Empire you know 2000 years ago were better off than the Europeans in the dark ages which came you know later right so growth wasnt always system so sustained even if you had higher higher living standards at one point in time your society might go backwards. We call it growth is compound I geometric growth or exponential growth. This means that small differences in average annual growth rates lead to large differences in GDP per capita over time. So were going to take 1.02 to the 100 And that means 100 years hence it will end up with GDP per capita of a little bit over 7000 Country be will take that same 1000 but its going to have average annual growth of 4% How will it do its going to end up with GDP per capita of 50000 more than seven times the other country because on average its growth was higher. Can do their dividing The day after the midterm were all tired. I mean its So rapid and it happened on average to so many people. As Larry Summers who was the Treasury Secretary when he gave me he said when they write the history of the 20th century sure. So this is this is from last January it was very current when Get all hadnt been affected so much by then or at least didnt know they had so Chinas slowing growth underlying stress facing its economy and 2020 On Friday Chinese officials said the country emerged from with an official economic growth rate of 6.1% Well within the governments target range of 6% to 6.5% but the lowest level nearly three decades. Last years growth rate however was well below Chinas 9.5% average Annual expansion between 1978 and 2017 notice thats it. All right so why do nations become rich So a couple hundred years ago it was not so obvious that countries could increase their living standards much I showed you that graph before that showed you know Italy had its little blip in the Renaissance but then kind of went back down. But I think its really important for For giving you perspective so that you dont take growth for granted. Okay so first were going to fill in a few necessary tools including some material you studied in Chapter 13 of Econ one so itll get those cobwebs out from there. Okay you get the marginal product of that workers pretty high because without them. They dont come to the ice cream shop and so therefore you cant make a sale if you add a second worker. What do I call it a PL book called the ALP so I will call it that. Right recall from Chapter 16 that average labor product ALP is equal to total output divided by the number of workers or if you want to count up hours per worker it could be total output divided by the number of hours worked. This graph on the horizontal axis has labor input for example that could be the number of workers or it could be the number of hours. This is called a production function right which tells you how much output you get first certain level of inputs here the input were focusing on is labor and if theyre using any other inputs like capital were holding those constant as we go along this craft. There at point A that tells us if we add this much labor. If you guys still do that school same here at be but notice You get less of an increase in output for a given increase in labor. ',\n",
       "             1: 'Okay so so it might be just because what we were just fielding all these emails this issue and that. Okay so most macro series grow at what we call compound rates. And so yeah were going to make it be output real GDP. And if you start at 200 youll end up at almost 310 years from now. No theres no way you had that on the midterm we check that over and over again. It was on a review discussion section or one of those questions and I forgotten to take it off of there. So you know once we start doing continuous time we write everything as a function of time. Supposed to a is equal to be time see times d. And at each series say is growing the time although it could also be decreasing. This ones growth rate of chiefs of deep Well if we then write a of t equals to all of these things a product of all of these things. Lets see 3% Or does anybody want to tell me why they chose 4% exactly perfectly explained GDP per capita. What was GDP per capita you can do that in your head. However its just much more useful to just put everything on an average annual basis because that way you can compare across countries you can compare across episodes with different numbers of years in there. And why it is that weve experienced this and why it is that some countries have been left out. Why is it that some countries are growing so much faster than other countries or have higher standards of living. Okay so in 1870 In 1870 The UK was one of the wealthiest countries in the world in terms of the major countries. Do us was second in terms of this chart and these countries here Japan was only only 737 per capita China 530 India 530 300 439 Per capita GDP and 19 $90 the UK was still doing quite well. So for a lot of countries and this is the 1300 to 1800. And there was you know there was of course the amazing flourishing of art and and those sorts of things that we still have with us and theyre estimated GDP per capita didnt go up some But notice it fell back down here and just kind of good. You know so so you would have these rises but then kind of go down and then you know another this is a little uptick with Portugal because of what was going on in the new world but then that started going down. This is why countries really sweat 3% like for the US 3% growth versus 2% growth. According to this rule we take 72 thats always in the numerator and we divide by three because we want it in the percent terms and that is equal to 24 years So that means it takes 24 years To double And its approximate So this is another really useful approximation so that you can just do things in your head without having to do a bunch of exponential creation. How many years did it take for Chinas real GDP per capita to double. The answer yes is 10 years Since thats approximately 72 over 7.1 Right. And I know youre at least a third of the students in this classroom in China right now so they they Know about this. Never before in the history of man man and womankind have so many people been lifted out of poverty into a middle class or even you know wealthy existence. So it was even large over that whole period according to the state regime was news agency. Major government policy for this year includes finalizing a decade long push to double income levels and they mean GDP per capita and the size of the economy. So the Chinese government really cares about numbers like how they double income levels how long it takes. So what Im going to talk about here is not in the textbook. Declines in the short run as the quantity of labor workers or hours or whatever employed rises. As labor input rises meaning as you add more workers or as you add more hours of work output also rises. Now the marginal product of labor is greater at point A than it is at point B. ',\n",
       "             3: 'There are some advantages other than worrying about the technical stuff to being able to take exams online. Okay so the first tool is how to calculate average annual company growth rates. And this is useful because often we want to summary statistic of one of the examples well talk about is China how much has been growing on average per year. Well you can see that the way we could do that is simply take that initial output multiply it by one plus a phrase that growth rate squared. We multiply this out twice first here then that why one already has that one plus A in it. Okay so then the cap t would be 10 and we would raise 1.04 to 10 Right so Obviously Im always thinking about 10 what heres an example. All right so Ill put in your 10 is equal to 200 times 1.04 to the 10th and thats equal to 296.05 so if you have 10 years where growth is 4% per year. Okay so well increased almost 50% Okay now suppose I tell you okay Im going to think about it the reverse. And thats going to be equal to 3207 So you need to solve for A. Yes we have to take the one 20th power of both sides. So thats just a one there equals 3207 Over 1000 To the one 20th. Okay and then whatever that number is subtract one and thats the A that youll get. And if you do that youll see youll get an 8.06 Which means 6% average annual growth. And if you use second before you use why the x it means its why to the one over x. So its very easy to calculate on it on a simple scientific notation calculator. Alright so Suppose you calculate the total percent change and divide by the number of years. But if you were trying to calculate the average annual one by dividing by the number of years it would imply 11% per year. This is not required but but a lot of plenty of people know about exponential growth. So again you do not need to know this but Ive just proved to you that it holds exactly for exponential for the other cases where were not compounding continuously but instead say every year. So lets do a practice problem and I have a poll Lets see. Nobody was answering 4321 All right we havent neck and neck between 3% and 4% Does anybody want to tell me why they chose. So that is why its 4% Not 3% Okay cuz somebody other people were Doing the division. Okay and what do we call the growth rate of price level. Alright practice problem three heres heres a two step or if the CPI is 1.5 and 2008 and 1.83 and What was the average annual inflation rate between 2008 and 2018. So in this case youre going to need to take a to Use a calculator that has exponentially ation. Why am I not Somehow I dont have this is just okay Im just going to do this for you because I see that I didnt do a poll for it. Alright so we have 1.5 that were starting with With the CPI and were going to figure out on average what was the average annual inflation rate over those 10 years. Now growth wasnt always so staying so sustain and you should never take growth for granted. And there wasnt much happening now Holland did manage to go up and kind of stayed on an upward trend which was unusual and then England was the other one. How could that possibly be so much different after 100 years in terms of the level. So heres a practice problem and I have a couple for that. Okay so Im in the poll you only have to answer for China. This is in purchasing power parity 19 $90 China in 1980 versus 2010 in India in 1980 versus 2010 so go ahead and see if you can figure out what the average annual growth rate of China actually was between 1980 and 2010 Okay. Um but 70% of the people got the right answer because what they did was for China. Im skipping those steps of algebra and then you subtract one and you get A point oh seven one which implies approximately 7.1% for China. And thats how they increase their GDP per capita by eight times over this 30 year period India increases GDP per capita but not as rapidly. This is the number of years it takes for the level of a geometrically growing variable to double. But but thats one that you could probably Google and find the answer to because that Yeah thats a really I should know that. Can you imagine living in a society where on average your household income. Alright so China has you know over a billion people 1.4 now. But one of the biggest stories about the 20th century has got to be the rise in China. So there was a very famous English economist known as Thomas Malthus and in a book he wrote in 1798 He argued that population growth would lead simply to more misery. Okay so were going to talk about the marginal product of labor and the average labor productivity. All right so if you when you add suppose youre at the ice cream shop. But suppose that there are quite a few lines that if people see lines. They will increase your sales but they will increase them as much extra as that first worker did so its that same kind of marginal product. Im now going to show how marginal product labor relates to the average labor productivity which thats what the book costs of the ALP I would call it. Okay its positive but at each level its going up less We can represent that with this graph. I know that sometimes the transcription of the thing doesnt transcribed correctly so I can imagine in number. '})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters = defaultdict(str)\n",
    "labels = [np.array(lda_model.get_document_topics(sent))[:, 1].argmax() for sent in corpus]\n",
    "confidences = [np.array(lda_model.get_document_topics(sent))[labels[i],1] for i, sent in enumerate(corpus)]\n",
    "for i in range(len(filtered_sents)):\n",
    "    clusters[labels[i]] += filtered_sents[i] + \" \"\n",
    "for topic in clusters:\n",
    "    clusters[topic] = re.sub(r\"\\s+\", \" \", clusters[topic])\n",
    "    clusters[topic] = re.sub(r\"[',]\", \"\", clusters[topic])\n",
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "contrary-cable",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-21 04:55:04.323 INFO    numexpr.utils: NumExpr defaulting to 4 threads.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>confidence</th>\n",
       "      <th>sentences</th>\n",
       "      <th>questions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.979228</td>\n",
       "      <td>The 200 and we have 4% growth than by your one...</td>\n",
       "      <td>{'answer': '4%', 'question': 'How much growth ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.956942</td>\n",
       "      <td>Major government policy for this year includes...</td>\n",
       "      <td>{'answer': 'a decade long push to double incom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.955185</td>\n",
       "      <td>Now youve heard about the law of diminishing r...</td>\n",
       "      <td>{'answer': 'the marginal product of labor', 'q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.957396</td>\n",
       "      <td>This is in purchasing power parity 19 $90 Chin...</td>\n",
       "      <td>{'answer': '1980', 'question': 'In what year w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.959360</td>\n",
       "      <td>What does that mean okay it means We know that...</td>\n",
       "      <td>{'answer': 'standards of living', 'question': ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   topic  confidence                                          sentences  \\\n",
       "0      0    0.979228  The 200 and we have 4% growth than by your one...   \n",
       "1      1    0.956942  Major government policy for this year includes...   \n",
       "2      2    0.955185  Now youve heard about the law of diminishing r...   \n",
       "3      3    0.957396  This is in purchasing power parity 19 $90 Chin...   \n",
       "4      4    0.959360  What does that mean okay it means We know that...   \n",
       "\n",
       "                                           questions  \n",
       "0  {'answer': '4%', 'question': 'How much growth ...  \n",
       "1  {'answer': 'a decade long push to double incom...  \n",
       "2  {'answer': 'the marginal product of labor', 'q...  \n",
       "3  {'answer': '1980', 'question': 'In what year w...  \n",
       "4  {'answer': 'standards of living', 'question': ...  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame().assign(topic=labels, confidence=confidences, sentences=filtered_sents)\n",
    "top_sentences = df.groupby(\"topic\").apply(lambda grp: df.loc[grp['confidence'].nlargest(1).index])\n",
    "top_sentences = top_sentences.assign(questions=top_sentences[\"sentences\"].apply(lambda sent: get_most_coherent_qa(qg(sent))))\n",
    "top_sentences.index = top_sentences.index.droplevel(0)\n",
    "top_sentences = top_sentences.reset_index(drop=True)\n",
    "top_sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrative-tooth",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "featured-meeting",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "questions = {}\n",
    "for i in range(num_topics):\n",
    "    questions[i] = {\"questions\":top_sentences[top_sentences[\"topic\"] == i][\"questions\"].to_list()}\n",
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "better-typing",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tanve\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\tanve\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "ename": "InternalHashError",
     "evalue": "module '__main__' has no attribute '__file__'\n\nWhile caching the body of `init_models()`, Streamlit encountered an\nobject of type `builtins.function`, which it does not know how to hash.\n\n**In this specific case, it's very likely you found a Streamlit bug so please\n[file a bug report here.]\n(https://github.com/streamlit/streamlit/issues/new/choose)**\n\nIn the meantime, you can try bypassing this error by registering a custom\nhash function via the `hash_funcs` keyword in @st.cache(). For example:\n\n```\n@st.cache(hash_funcs={builtins.function: my_hash_func})\ndef my_func(...):\n    ...\n```\n\nIf you don't know where the object of type `builtins.function` is coming\nfrom, try looking at the hash chain below for an object that you do recognize,\nthen pass that to `hash_funcs` instead:\n\n```\nObject of type builtins.function: <function init_models at 0x0000023AB8D2FA68>\n```\n\nPlease see the `hash_funcs` [documentation]\n(https://streamlit.io/docs/caching.html)\nfor more details.\n            ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\envs\\dsc\\lib\\site-packages\\streamlit\\hashing.py\u001b[0m in \u001b[0;36mto_bytes\u001b[1;34m(self, obj, context)\u001b[0m\n\u001b[0;32m    343\u001b[0m             \u001b[1;31m# Hash the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 344\u001b[1;33m             \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mb\"%s:%s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_to_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    345\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dsc\\lib\\site-packages\\streamlit\\hashing.py\u001b[0m in \u001b[0;36m_to_bytes\u001b[1;34m(self, obj, context)\u001b[0m\n\u001b[0;32m    599\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 600\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_file_should_be_hashed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__code__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mco_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    601\u001b[0m                 \u001b[0mcontext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dsc\\lib\\site-packages\\streamlit\\hashing.py\u001b[0m in \u001b[0;36m_file_should_be_hashed\u001b[1;34m(self, filename)\u001b[0m\n\u001b[0;32m    378\u001b[0m         return file_util.file_is_in_folder_glob(\n\u001b[1;32m--> 379\u001b[1;33m             \u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_main_script_directory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    380\u001b[0m         ) or file_util.file_in_pythonpath(filepath)\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dsc\\lib\\site-packages\\streamlit\\hashing.py\u001b[0m in \u001b[0;36m_get_main_script_directory\u001b[1;34m()\u001b[0m\n\u001b[0;32m    683\u001b[0m         \u001b[1;31m# script path in ScriptRunner.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 684\u001b[1;33m         \u001b[0mmain_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m__main__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__file__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    685\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmain_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module '__main__' has no attribute '__file__'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInternalHashError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-d6f4337b4498>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlp\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\SDHacks2021\\src\\backend\\nlp.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[1;33m@\u001b[0m\u001b[0mst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0minit_models\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mglobal\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mqg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msummarizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dsc\\lib\\site-packages\\streamlit\\caching.py\u001b[0m in \u001b[0;36mcache\u001b[1;34m(func, persist, allow_output_mutation, show_spinner, suppress_st_warning, hash_funcs, max_entries, ttl)\u001b[0m\n\u001b[0;32m    503\u001b[0m         \u001b[0mhash_funcs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhash_funcs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    504\u001b[0m         \u001b[0mhash_reason\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mHashReason\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCACHING_FUNC_BODY\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 505\u001b[1;33m         \u001b[0mhash_source\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    506\u001b[0m     )\n\u001b[0;32m    507\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dsc\\lib\\site-packages\\streamlit\\hashing.py\u001b[0m in \u001b[0;36mupdate_hash\u001b[1;34m(val, hasher, hash_reason, hash_source, context, hash_funcs)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[0mch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_CodeHasher\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhash_funcs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m     \u001b[0mch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhasher\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dsc\\lib\\site-packages\\streamlit\\hashing.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, hasher, obj, context)\u001b[0m\n\u001b[0;32m    367\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhasher\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[1;34m\"\"\"Update the provided hasher with the hash of an object.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 369\u001b[1;33m         \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    370\u001b[0m         \u001b[0mhasher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    371\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dsc\\lib\\site-packages\\streamlit\\hashing.py\u001b[0m in \u001b[0;36mto_bytes\u001b[1;34m(self, obj, context)\u001b[0m\n\u001b[0;32m    356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 358\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mInternalHashError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    360\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dsc\\lib\\site-packages\\streamlit\\hashing.py\u001b[0m in \u001b[0;36mto_bytes\u001b[1;34m(self, obj, context)\u001b[0m\n\u001b[0;32m    342\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    343\u001b[0m             \u001b[1;31m# Hash the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 344\u001b[1;33m             \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mb\"%s:%s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_to_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    345\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    346\u001b[0m             \u001b[1;31m# Hmmm... It's possible that the size calculation is wrong. When we\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dsc\\lib\\site-packages\\streamlit\\hashing.py\u001b[0m in \u001b[0;36m_to_bytes\u001b[1;34m(self, obj, context)\u001b[0m\n\u001b[0;32m    598\u001b[0m             \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhashlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"md5\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 600\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_file_should_be_hashed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__code__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mco_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    601\u001b[0m                 \u001b[0mcontext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__defaults__\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dsc\\lib\\site-packages\\streamlit\\hashing.py\u001b[0m in \u001b[0;36m_file_should_be_hashed\u001b[1;34m(self, filename)\u001b[0m\n\u001b[0;32m    377\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m         return file_util.file_is_in_folder_glob(\n\u001b[1;32m--> 379\u001b[1;33m             \u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_main_script_directory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    380\u001b[0m         ) or file_util.file_in_pythonpath(filepath)\n\u001b[0;32m    381\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dsc\\lib\\site-packages\\streamlit\\hashing.py\u001b[0m in \u001b[0;36m_get_main_script_directory\u001b[1;34m()\u001b[0m\n\u001b[0;32m    682\u001b[0m         \u001b[1;31m# This works because we set __main__.__file__ to the report\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    683\u001b[0m         \u001b[1;31m# script path in ScriptRunner.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 684\u001b[1;33m         \u001b[0mmain_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m__main__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__file__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    685\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmain_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInternalHashError\u001b[0m: module '__main__' has no attribute '__file__'\n\nWhile caching the body of `init_models()`, Streamlit encountered an\nobject of type `builtins.function`, which it does not know how to hash.\n\n**In this specific case, it's very likely you found a Streamlit bug so please\n[file a bug report here.]\n(https://github.com/streamlit/streamlit/issues/new/choose)**\n\nIn the meantime, you can try bypassing this error by registering a custom\nhash function via the `hash_funcs` keyword in @st.cache(). For example:\n\n```\n@st.cache(hash_funcs={builtins.function: my_hash_func})\ndef my_func(...):\n    ...\n```\n\nIf you don't know where the object of type `builtins.function` is coming\nfrom, try looking at the hash chain below for an object that you do recognize,\nthen pass that to `hash_funcs` instead:\n\n```\nObject of type builtins.function: <function init_models at 0x0000023AB8D2FA68>\n```\n\nPlease see the `hash_funcs` [documentation]\n(https://streamlit.io/docs/caching.html)\nfor more details.\n            "
     ]
    }
   ],
   "source": [
    "from src.backend.nlp import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "serious-jacksonville",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-21 04:21:23.275 INFO    transformers.configuration_utils: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/sshleifer/distilbart-cnn-12-6/config.json from cache at C:\\Users\\tanve/.cache\\torch\\transformers\\90dbb6d4e8ba2227a79a0e5cc64eb216091a1d5c362a83c52c6c242a44d89b50.e1484cfc83ca98dbf8dbb660097e95a5ef503f348dd354064e58fa28f5e3ae16\n",
      "2021-02-21 04:21:23.276 INFO    transformers.configuration_utils: Model config BartConfig {\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"force_bos_token_to_be_generated\": true,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"length_penalty\": 2.0,\n",
      "  \"max_length\": 142,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"min_length\": 56,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \" \",\n",
      "  \"replacing_rate\": 0,\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"student_decoder_layers\": null,\n",
      "  \"student_encoder_layers\": null,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4\n",
      "    }\n",
      "  },\n",
      "  \"vocab_size\": 50264\n",
      "}\n",
      "\n",
      "2021-02-21 04:21:23.277 INFO    transformers.tokenization_utils_base: Model name 'sshleifer/distilbart-cnn-12-6' not found in model shortcut name list (facebook/bart-base, facebook/bart-large, facebook/bart-large-mnli, facebook/bart-large-cnn, facebook/bart-large-xsum, yjernite/bart_eli5). Assuming 'sshleifer/distilbart-cnn-12-6' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "2021-02-21 04:21:24.522 INFO    transformers.tokenization_utils_base: loading file https://s3.amazonaws.com/models.huggingface.co/bert/sshleifer/distilbart-cnn-12-6/vocab.json from cache at C:\\Users\\tanve/.cache\\torch\\transformers\\c43535d7925e8df7929ff33258438f051b865efeb565a524105194edd7543aa2.6a4061e8fc00057d21d80413635a86fdcf55b6e7594ad9e25257d2f99a02f4be\n",
      "2021-02-21 04:21:24.522 INFO    transformers.tokenization_utils_base: loading file https://s3.amazonaws.com/models.huggingface.co/bert/sshleifer/distilbart-cnn-12-6/merges.txt from cache at C:\\Users\\tanve/.cache\\torch\\transformers\\3d48487241499d5bcbcae41319e70e8e838f79b2ea1e8df5301e362d70fe3664.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "2021-02-21 04:21:24.523 INFO    transformers.tokenization_utils_base: loading file https://s3.amazonaws.com/models.huggingface.co/bert/sshleifer/distilbart-cnn-12-6/added_tokens.json from cache at None\n",
      "2021-02-21 04:21:24.524 INFO    transformers.tokenization_utils_base: loading file https://s3.amazonaws.com/models.huggingface.co/bert/sshleifer/distilbart-cnn-12-6/special_tokens_map.json from cache at None\n",
      "2021-02-21 04:21:24.524 INFO    transformers.tokenization_utils_base: loading file https://s3.amazonaws.com/models.huggingface.co/bert/sshleifer/distilbart-cnn-12-6/tokenizer_config.json from cache at C:\\Users\\tanve/.cache\\torch\\transformers\\0731a7027a83bcc6caa8200c7ed255653dbb2ea38a82a49b783401aa6ecfd028.d596a549211eb890d3bb341f3a03307b199bc2d5ed81b3451618cbcb04d1f1bc\n",
      "2021-02-21 04:21:24.525 INFO    transformers.tokenization_utils_base: loading file https://s3.amazonaws.com/models.huggingface.co/bert/sshleifer/distilbart-cnn-12-6/tokenizer.json from cache at None\n",
      "2021-02-21 04:21:24.805 INFO    transformers.modelcard: Model card: {\n",
      "  \"caveats_and_recommendations\": {},\n",
      "  \"ethical_considerations\": {},\n",
      "  \"evaluation_data\": {},\n",
      "  \"factors\": {},\n",
      "  \"intended_use\": {},\n",
      "  \"metrics\": {},\n",
      "  \"model_details\": {},\n",
      "  \"quantitative_analyses\": {},\n",
      "  \"training_data\": {}\n",
      "}\n",
      "\n",
      "2021-02-21 04:21:25.015 INFO    transformers.configuration_utils: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/sshleifer/distilbart-cnn-12-6/config.json from cache at C:\\Users\\tanve/.cache\\torch\\transformers\\90dbb6d4e8ba2227a79a0e5cc64eb216091a1d5c362a83c52c6c242a44d89b50.e1484cfc83ca98dbf8dbb660097e95a5ef503f348dd354064e58fa28f5e3ae16\n",
      "2021-02-21 04:21:25.016 INFO    transformers.configuration_utils: Model config BartConfig {\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"force_bos_token_to_be_generated\": true,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"length_penalty\": 2.0,\n",
      "  \"max_length\": 142,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"min_length\": 56,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \" \",\n",
      "  \"replacing_rate\": 0,\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"student_decoder_layers\": null,\n",
      "  \"student_encoder_layers\": null,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4\n",
      "    }\n",
      "  },\n",
      "  \"vocab_size\": 50264\n",
      "}\n",
      "\n",
      "2021-02-21 04:21:25.238 INFO    transformers.modeling_utils: loading weights file https://cdn.huggingface.co/sshleifer/distilbart-cnn-12-6/pytorch_model.bin from cache at C:\\Users\\tanve/.cache\\torch\\transformers\\14e37ad7876e80795e5bf0f13ddc7b83a8fb8dff8ef857451052bc2e34797003.71e977223cb272c823f1ee09496c0b291b1f230435f303707d15dcc6cfe9071f\n",
      "2021-02-21 04:21:34.029 INFO    transformers.modeling_utils: All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "2021-02-21 04:21:34.030 INFO    transformers.modeling_utils: All the weights of BartForConditionalGeneration were initialized from the model checkpoint at sshleifer/distilbart-cnn-12-6.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "summarizer = pipeline(\"summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proof-universal",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_sentences = df.groupby(\"topic\").apply(lambda grp: df.loc[grp['confidence'].nlargest(10).index])\n",
    "summary_sentences.index = summary_sentences.index.droplevel(0)\n",
    "summary_sentences = summary_sentences.reset_index(drop=True)\n",
    "to_summarize = summary_sentences.groupby(\"topic\")[\"sentences\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tutorial-court",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = to_summarize.apply(lambda x: ' '.join([t[\"summary_text\"] for t in summarizer(x)])) \n",
    "summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cheap-shame",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"topic\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "elementary-insight",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-9223371883677288780"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash(qg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "demographic-building",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-9223371883686807116"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash(summarizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "magnetic-landing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-9223371883709446924"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "automated-butterfly",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-9223371883677284784"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "banner-finnish",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-953f854a5cee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhash\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "hash(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "connected-tsunami",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140708041546976"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuffed-basin",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
