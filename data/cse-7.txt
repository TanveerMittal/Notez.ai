Okay, good morning everyone let's get started i'm starting to record. You just. mute everybody if that's okay. Okay, so let's start with some. questions. Anybody have any questions or comments. Everybody know what's going on with the. exam and everything there's no homework do this week so we'll be releasing the next homework. On Wednesday. And then it'll be do the following Wednesday. Get back right on schedule the exam is going to be. start at 8am. Friday. till 8am. Saturday it'll be a timed. 80 minutes. it's gonna be great scope. Online assignment. hasn't been finished that hasn't been written. or hasn't been finalized, so I can't tell you how many questions. um so i'm going to encourage you to all just type your answers into the prompt, but I will give an option to upload files if that's what you prefer. Okay let's get started, then. So today we're going to shift gears a bit and talk about encoding but we're going to see that. Did you not hear the answer. That incoming is very closely related to counting. which is what we kind of just did so. we're going to look at encoding in general, then we're going to look at encoding strings, which is a very common type of object that we need to encode and then we're going to look at fixed length encoding strings or variable length and coding. Alright, so indiscreet math class in CSE. In this university but discrete math classes anywhere you take them you're going to be dealing with math objects right, and so what our math objects. Really kind of anything we use in math it could be like a geometric shape or a graph or just integers themselves binary strings of course other strings right we did or depending on who you took it with you did a lot of string stuff with 20. Of course, sets sets and subsets we just went over permutations and cycles right and combinations I should put those in there right combinations. But even like kind of things that. That were very specific to the class like domino tailings or night arrangements like these things are also basically math objects. Now, when we're in class and we're talking about them, we have our own notation to write them do you think that's the best way to store them in a computer maybe maybe it's the best way to store it in the computer, because then, when you look at it, you know you, you understand the. The notation but maybe it's not because it's takes up too much space, so now, you have to have some sort of balance of is there a better, more efficient way to store it. And what are you giving up by doing that. Okay, so in computers, we use bits to store data so that's what we're going to encode things by. If this data comes in the form of one of the previous math objects in the process of translating an object or datum into bit strings is called encoding but computers are encoding all the time they encode of course text right all your text messages all the chat messages. All your emails just basic websites that are mainly text right. Images of course all the images that you see on the Internet. Images like this image that I have here as my as my slide. Video, of course, like the video that i'm presenting to you now it's going to be recorded is going to be stored in the computer as bits now there are kind of inefficient ways and efficient ways to do that. Of course program sound right music. Of course math. object so. Anything coded into a binary string. Now there's different kinds of encoding there's something called lossy encoding. Some of the data is lost in the encoding process so that means that if you have an original object, when you encode it with a lossy encoding there's no way that you can get the original object back what is lossy encoding good for does anybody know. audio. Video. Right so anything that really. That the user interacts with with it with his or her senses is usually okay to do some awesome encoding the reasons, because our eyes are not perfect, our ears are not perfect. You know if you give me an image and you compress it right, you make it smaller. you're losing a lot of the details of that image, but maybe some of those details I don't care about maybe i'm just looking at the image to. I don't know study I don't know a map or something right and maybe all I really need to know is, you know the general shape or the general roads, I don't need to know that there's like trees here or mountains there. So, in certain circumstances it's okay to lose information in this class we're not going to do that, that those that's for kind of different classes. we're going to do something called lossless encoding, which means that will after you encoded into binary you can. there's enough information in the encoding for you to reconstruct. The original object and that's going to be important for other things right. programs. text right. And just kind of in general strings and other things that you don't you don't want to you don't want it to be corrupted you don't want anything to be wrong with it, you want all the information to be accessible. Okay, and we're going to talk a little bit about something called an error detecting or error correcting code and coatings we're going to spend a lot of time on it, but I thought I would mention it, because they are kind of cool. And it has to do with if your encoding is slightly modified right if you flip only a few bits, then the error can be detected or corrected, and this is really good for a lot of applications, because we don't always have a reliable network line. It could kind of be affected by outside noise or even like malicious software or malicious people. Okay lossy compression compression. If you have an image described as a grid of pixels right. The idea is that you can use linear algebra and we're not going to go over the way that you do this, I think you I think there's another class that does this image processing class. And you can compress it down to a fraction of the size, so what you gain in saving space, you lose in details so. If you compress the image down this much you can kind of see it sort of grainy and gritty Whereas the original image is is much more clear all the details are more well defined, but even going from here to there. At least for me I can't discern any difference, you know my eyes aren't good enough to figure that out so maybe it's fine to compress it down a little bit. Okay, you can save a lot of space how about videos or videos the the traditional way to. To show a video right when you're looking at a film, they have a different image for every frame now if you're storing images on your computer. Images take up a lot of space, so there are video compression algorithms that detect certain areas that don't change or change very little so that they can recycle the same data over and over again, for example. This little. patch of sky. doesn't change right, and so you can you don't have to. You don't have to store it eight times you can kind of just store it once or twice and and reuse it and there's there's. tons of. Great algorithms that do that, but of course that's lossy encoding because the cloud could have moved a little bit and you didn't detect it, but nobody really cares everybody's watching the golfer. Okay error correcting codes what of somewhere along the line, some of your bits changes or, even worse, new BITs are introduced, or some of your bits are deleted. This could be catastrophic depending on how you encoded your data right. So you have your original data to encode it and you stored in your computer, but maybe something happened along the way, or maybe you dropped your computer or something. Or it was in close proximity to like a high powered magnet. or when you started to decode it the there was some sort of. I don't know process that got changed, and then you get your data back, but it might not be your original data. But there's a way around this there are these things, called error correcting codes. The simplest example is something called a repetition code and you could do a repetition with with any number. doing it with with. Repeating every digit if you repeat every digit one time then that's just kind of the regular code, if you repeat it two times, then it's not really that helpful, but if you repeated three times, then maybe you can. You can actually gain the original information back and, of course, if you're four or five or six times is much less susceptible to. Two changes, but on the other hand, it is a lot more space that you need so there's a balance here right, if you want. Something that is less susceptible for for errors, then you need more space, but if you want something that is more is a smaller space that is going to be more susceptible to errors is kind of the general trade off. So, for example, this code I don't know what it's encoding but 0110 would encode to write you kind of have these blocks of three. Right what types of errors with this detect and what types of errors, would it not be able to detect so of course if I change any single bit right if I change any single bit let's say I changed this bit to a one right. If you change a single bit it can actually detect it because it will it will know, to read three bits in a row, and it will see this, and it will know that that one was changed because there's more zeros and ones right. What kind of errors, would it not be able to detect. Right to bit flips that are right next to each other. So if you go like one one now you don't really know how to get back the original thing. How about if you insert a bit. Right, what if you inserted a bit here. You might still be able to kind of figure out what the original thing was right, what if you didn't write bit deletion, what if you deleted a bit right, what if you deleted that bit there, it might kind of shift everything and maybe maybe mix you up a little bit, but. But this is a really nice. nice way to. detect errors, but it's really inefficient there's much better ways to do it, and I think that there's a class that deals with that has anybody taken any of those classes before that you've ever talked about error correcting codes. Something called the what is it. called the hamming code. For you all ever seen this stuff before. Yes, error detecting. Oh, you can have like a parody bit that's kind of a cool thing. cryptographic hash function. yeah So what are the applications a practical applications is like anytime you send data to any time of network, there is a possibility that your code your your. data is going to be corrupted and there's sort of three types of ways, it can be corrupted it could have a bit change. right we changes from zero to one or one to zero, you have a bit edition, where you insert a bit somewhere or but bit deletion, where you delete a bit and you could have some you know, depending on how a. How susceptible your network is, you can have a combination of a bunch of those things, so the idea is, is there a way to kind of mitigate that at. Those errors, is there a way to detect them and or correct them. isn't that the case, all the time. What that there, there is a possibility of. yeah it's it's always a fear that that's going to happen, you know but and there's a bunch of. there's a bunch of. ideas that people have that will I mean this is just one of them, this is kind of like a simple one, but there are much more complicated ways to do it. That are much more efficient right you don't need to multiply the number of bits by three right there's one that i'm trying to find what it's called. kind of forgot. It basically uses more bits than you need as kind of redundancy bits to kind of tell you what's going on right. But another way to do it is to take a cryptographic hash function and send the result of the hash function, along with the code, so that when you receive the code. You you do the same hash and make sure the hashes are equal, the ashes are equal, then you know the code is probably good that's that's like what they do a lot of the times with like credit card information. Okay anyhow let's move on. here's another error correcting code, it is something that you may have seen before. it's called a two out of five code and I wanted to put it here because it deals with this combinations that we've. That we've talked about five choose to is equal to 10 so what some people have done is they have. assigned each digit 0123456789 to a unique fixed density binary string with exactly two ones. And you can see it in your letters they use this. and Instead of using binary they use these kind of like short and long things well that's Nice, I think I think it's that this. can't draw on that. I think it's that this first one is is ignored, so you have short short long short long right so that's too too short or long short long right because you can see that again short short long short long right and then zero must be long long short short short. Right and so. This can detect errors, because a it's just like, why is this error detection. I guess you're just using more bits because it's more redundant. You know that there has to be too long bits somewhere so. If one of them gets cut off, then. Anyway, I thought it was cool because it has something to do with combinations. Anyhow. So let's talk about what we're going to do in this class we're all that stuff we're not really going to concern ourselves with I just thought it was nice to start with. we're going to look at something called lossless data compression and so you have your original data, you have your encoding algorithm you can store it in your computer and then you have a decoding algorithm that you can get your original data back. And we're going to assume that there's no. there's no susceptibility to. bit changes. Okay, what does it mean for an encoding to be efficient, well there's two things, and they and they kind of opposite so when we talk about efficient codes i'm mainly going to be concerned with the length of the code or the space. Space efficient. But it's worth it to to know about time all the time it takes to encode and decode also you can talk about time efficiency. But that's why I put this scale here is because. In general, if it's more space efficient. That means the encoding algorithm is more complicated and if it's more if it's less space efficient than the encoding algorithms is less complicated means that it's faster right so. These things balance out, what do you want it depends on what you're doing if you're accessing these things, all the time you probably want a very simple encoding algorithm you're going to lose a lot of space you're going to save a lot of time and then the other way around. Okay let's look at an example. If you have like a genomic code right that's basically a string of four characters. In the 1990s, when they first started using computer well not first started using computers for DNA but kind of when it kind of blew up and started they started doing it much more often. They used something called ascii. To code these strings which in hindsight seems kind of silly because ascii is a eight bit code. That each each letter or symbol that your keyboard can type, there is an associated eight bit ascii code so, for example, a has this code here si T and G, and so you need eight bits for each letter. So after they after they had tons and tons of storage full with these ascii code DNA sequences. They had to save space somewhere, now the The easiest way to go, is to instead use binary and use two bits per letter right because there's four letters so since there's four letters, you only need two bits to uniquely identify every letter. Or, and whatever and let her code is going to be two n bits rather than eight in bit so you save you know by a factor of four which is incredibly. Okay, so let's just talk about strings in general, when talking about strings over a finite alphabet, for example, yeah binary strings well, those are probably those are arguably the easiest things to encode. And the reason is because you just don't do anything now ternary strings they have three different characters euro one and two, we have quite ternary strings DNA strings decimals strings hexadecimal sex adjustable which is 6060 characters. alpha numeric alphanumeric we're going to see that this is 92 characters if you include all of the 95 characters if you include all of the special symbols. So we saw that okay binary strings you need one bit per. character. DNA strings we saw that we needed to bits. per character. How about ternary strings. If we're talking if we're using a fixed length and coding, that means that every character is going to have the same number of bits so for fixed length and coding. How. Many bits are required for ternary strings. Okay, good to bits where did you get that number, do you have, is there a formula that you can use to calculate that. Okay excellent it's the ceiling of log base to have three. Okay, how about decimal strings. With a fixed length and coding, how many bits do you need. For right because four is equal to the ceiling of law based to have 10. And hexadecimal strings this is 16. So you also need for. Right. How about 60 characters you need. Six I suppose right. And for alphanumeric you need seven. Okay, so this is for each character, so that means that once you figure out the the the code for each character, then you can build a string by just putting those codes side by side. yeah okay here's a i'll just read what what the students said. Could we potentially get more bang for our buck with the ternary strings if we build some sort of coding that allows us to. Quote unquote pack more information into the two bit words we have four bits but only three unique combinations are needed to represent characters that fourth value could be used to pack additional information about future values, yes. So that's going to kind of be what we look at a little bit later something called a theoretical optimal encoding there is a you know, a. A lower bound on the number of bits required to use, and that has that has links to something called entropy has something to do with chaos, a random random. binary strings something to do with sort of the complexity of the object in a way, so all of these things have to do with what what is the minimum amount of information that I need to uniquely identify each thing, but if you do it that way. you're not using a fixed length encoding anymore so fixed length and coding, is a very basic nice way to do things it's it's essentially what we did with this binary I sorry DNA strings. it's really easy to encode and decode because all you have to do is. is just decode each each block and you know how long each block is so you don't need any delay limiters between them. Okay, great. Great observation anybody have any other comments or observations. Long blocks of ternary to integer that's going to actually be the optimal encoding for ternary strings is to convert them into their integer in based to that's the best way to do it, but that's not fixed density or fixed length and it's arguably a little bit more complicated because. It takes it takes more processing time to convert basis rather than just convert each block of two to whatever character, there is so hopefully again idea of you know. The differences. Okay, so integers integers are among the main building blocks of mathematics, especially with in the sub field of arithmetic which our computers are very good at doing. In our civilization we use base 10 numbers like 125 400 and 4013 and 72 How does a computer store these numbers does anybody know. Well, I guess i'll start by saying there's two ways right so like a. up more powerful computer. will convert. The number. In. Their binary expansion right, how do you call that binary power binary expansion binary number system. Then do arithmetic on that. Storage right. Then convert back. When. When ready. or when when when ready to print I guess. Because, as the user i'm going to plug in the numbers like I know them right 125 times 4013 go it converts both does the arithmetic with binary numbers and then converts the result back. Okay, this is actually the most efficient way to do it in a few different ways it's efficient with space right. it's space efficient. And the arithmetic. is more time efficient. Okay, so that's a powerful computer what's not a powerful computer something like this does anybody know how a sort of low power calculator stores integers. binary coded decimal okay so like a calculator. stores. Using dcd and B, C D all it is, is a fixed and it's a fixed length think coatings. So each digit is a is a four bit binary string right if we go back to here we can see that decimal strings require four bits if you want to use fixed density or sorry not fixed fixed length encoding so you need four bits per digit. So what are the advantages of this. Right, the encoding and the decoding are very fast right, so the encoding here and decoding. is slow. But the encoding is fast here. Could you really don't have to do any math all you have to do is is is look up on a table. Okay, so the BC D encodes each decimal digit to its binary equivalent B, C D is a fixed length and coding right, and this is how they do it. This is the dcd code right, so how would you represent 711 and B, C D for you just code each digit. So seven would encode to 0111 would encode to 00011 would encode to 0001 and so you have the string the binary strings 011100010001. And you don't need any delay limiters in between them, because you know each digit is exactly four bits long, so you know exactly where one starts and one ends and that's very important and that's one of the main. The main things that make fixed length encoding really good and really efficient. Okay, so, then we have you know, since there are 16 different four bit strings we have these forbidden. forbidden strings here, but you know if you're if you want to be clever, then you can. Maybe you can encode certain operations right if you wanted to store that plus minus times and divide and goals or something if you wanted to kind of display them all, you could you could use those forbidden things as other symbols, if you wanted to. But there's a whole way to do arithmetic using dcd it's not as efficient as just a rhythmic tick on binary numbers, but the. The time you're saving with encoding really pays off for these things because. it's it's a bit complicated, especially for big numbers to have to go back and forth between decimal and binary so now, when you need to display this thing the computer doesn't need a lot of. A lot of time to just say Oh, the first four bits I just look them up oh it's a seven the next four bits is a one and one okay any questions about that. So this is a fixed length encoding. Who is the CD basically anything that has like this type of display clocks digital ovens I don't know any kind of like low. Low power computer or sort of digital device. Okay, so text editors and programming languages, usually allow for strings or text over the alphabet, and this is the alphabet containing. 26 uppercase letters 26 lowercase letters 10 digits and 33 other symbols, including the space, so you add that all up and you get 95 how many bits are required for each character in this alphabet. Seven right. So you need seven bits per character so. The whatever universal international computing people who decide on these things develop this thing called the ascii code now here's the ascii table. And you can see that it has all of the uppercase letters all the lowercase letters and all of the digits and then it has a bunch of other stuff um. But notice that the ascii ascii code use eight bits. Now, why do they use a pitch does anybody anybody know there's if you if you look at this ascii table notice that the first bit is always zero right in all of these codes, so why use that extra bit Okay, so one reason is to allow for future extension which they did. anybody else. Any other reason is. right if this thing, where a bite is a more convenient size right seven is weird to story memory and so there's all these operations at the computer can already do on these words sized chunks of of a bite of eight bits. And so that extra bit you know it's worth it to just add it in there, so that your computer can can deal with these things better. yeah. um so also if you notice. You have all of these these three columns here. Are all the symbols and all of the. letters and numbers and everything and then this column here is like a bunch of commands don't really know what that is, I guess, I recognize escape. I don't really know what those are all right good I got recognized tab. Right and then you have delete here. that's where you get the 95 necessarily the symbols, the 95 symbols, but the whoever made this up. You know they they wanted to get their best bang for their buck right they wanted to make sure that they were using all of the things that that was that was available, so they found other other commands to put. Okay, then there's the extended ascii table. which has all these kind of letters with accents and stuff and. But we're not really going to work with that so much okay so um let's talk about a a way that you could think about a computer, to be able to store this encoding decoding information so one way to do it is to store it in something called the encoding decoding tree. So, given an alphabet with an symbols if you're using fixed length and coding at least login bits are required to encode each symbol we've already talked about that right. So if I have a alphabet with four. characters, then I only need four bits and a way that I can encode the information code or store the encoding algorithm is by using a tree like this So how do you read the tree. Well, you start from the root and then you go down to the leaf, so if I wanted to encode the letter b I would go down to be and I get a zero. So be encodes 201 and then to decode you just kind of do the same process if I see a 01 I can just follow the tree or follow the zero, and then the one, and I know it's a B so it's a way to encode and decode. Okay, and since the tree is. is full and complete right and balanced and everything and the the bottom level is fully filled, then you know that all of the paths from the root down to the leaf are the same length and so that's why you get that fixed length encoding. Okay, so there's something else called variable length and coding. And we've seen examples of fixed length and coding, where each symbol is encoded using the same number of bits if the symbols are roughly evenly distributed then fixed length and coding is really a great way to go. Otherwise, if some symbols are more frequent than others than a variable length and coding is better because you can the more frequent symbols, you can shorten their code. And the longer symbols, you can make their code longer, and this sort of trade off that you do will will make your code more space efficient. Okay, so here's an example of a variable length encoding now this doesn't really encode it into bits, but it does encoded into some sort of. More kind of digital form. Okay, so here's the. The general frequencies of letters. He has the highest frequency of around 12.7% and what has the lowest frequency Q, maybe. I guess Z is less frequent than Q. Right. And so, this this Morse code. is a way to encode all those letters and the way that it works is that. a.is just one unit of time and a dash is three units of time, the space between parts of the same letter is one unit, the space between the letters is three units and the space between words is seven units so. You you kind of get like I don't know how to read Morse code or listen to it, but people who do you know or people that do it, they have their own little rhythm and you can start hearing the spaces and stuff like that in fact I was reading something about how, when they use Morse code. People had their own kind of like accents where like if you're listening to a Morse code, you could sort of tell who, like the individual you could recognize his or her. rhythm and be like oh yeah that's miles doing the Morse code, so there is sort of a lot of variability in here, which makes it. susceptible right, but it was used a lot, so that you know, there must have been something good about it. Okay, but notice that he is the shortest thing it's just a single da and Z is very long right, it has three plus one space plus three plus one plus one plus one plus one so that's 123456 is 11. Right so it's like 11 times longer than the shortest one. Okay, so. Obviously you're not going to be doing a lot of z's you're going to probably do a lot more ease so that's why you have that variable length. So Morse code was not super efficient, since we also need to store the spaces in between the word in between the letters are words right so. Is there a way that we could eliminate those spaces. Consider the following and coding scheme, how would you encode the word dad. well. I have D so let's do it dad I need D and then a. So my question to you is. Could I just give you this string 10010 and have you be able to. reliably decode it or Do I need to actually give you. commas, or the commas necessary. Yes, right because. So I guess dad would encode to this thing, but this thing could decode to a be. A be or a bb a be right. or it could be, or it could decode dad or it could decode to do a. Oh hey is zero sorry this is be a a B, or it could decode to da BA right so it's like a many to one thing we kind of saw this before, maybe. Wear shirts easy to encode something, but in order to decode it, you have all these options which one do you do so, you need these comments now, how do you encode commas. You know, you could use ascii but then you're you're putting like this eight bit. chunk in between the word. It would be much better if we could figure out a way to get around using commas, just like we did with the fixed length and cody remember we didn't need comments, because we knew exactly how long each were. or each through excuse me, each character was where the less. Okay, so we want to do something called a comma free code a comma free P prefix free code is an encoding scheme in which no concatenation of two words to code words contains a valid code word that overlaps both. So this is the one that we just looked at this is bad. And I think somebody kind of said, it is that. Be starts with one. right but C and D also start with one, so if I if my first letter or my first character is a one, how do I know if it's a B, C or D. I guess, more importantly, is that B is just one is just the code word one, and so, if I start with a one, how do I know if it's just a B, or if it's going to be the start of a C or D. Okay, this one's good yeah this is fixed. length. Fixed length and coding is always going to be comma free prefix free and it's basically because you know exactly when each word is going to end okay how about this one is this one good or bad. it's good right any word that you have. You can always decode it and get the original The reason is because. All of these strings none or none of these strings are the prefix of another string. Good bad bad good bad good good Okay, so one way to kind of generate a nice variable length prefix free encoding is something called a huffman code. and it's actually the most efficient variable length encoding if you are, if you are given the frequencies so, on average, the number of dates required to encode to give a message will be shorter than a fixed length and coding scheme. It uses a binary tree like we saw before and different trees may be used for different messages. Okay, so you can look in the book in page 764 it gives you the procedure for building a huffman tree i'm just going to give you a i'm just going to show you an example and and hopefully that'll kind of give you the idea. Okay let's say we have a file with the following single letter frequencies. Okay, so a is the most rare and G is the most frequent so i'm hoping that in my encoding the code word for G is short and the Cold War, for a long. OK, so the way that we start is to. sort the frequencies in decreasing order. Now, how do you break ties let's just say. I guess what I did here is I just broke ties by. Reverse alphabetical order. Okay. So what's going to happen here. Is that we're going to take the two rarest. letters. And we're going to kind of. join them together. we're going to join them together and we're going to add their frequencies. To close, 123 the idea here is to set a or B right then that set is going to occur with frequency three so this sort of Sub tree has frequency three. And then, what you do is you put that sub tree back into the you resort it put it back into the list and then you repeat and you keep on doing that, until you build the whole tree. Okay, so that's sort of after step one right now step two is take the two rarest kind of characters in this case is going to be two and three. Right and connect those and add their frequencies, so now this subset ABC is going to occur with frequency five right so now reorder them. So now, the five is is more frequent than end so now let's put those ones together right and I get eight and now that's the most frequent so move that to the front. And so on right now we have 10. That to the front, we connect the six in the 814 that's the front and then connect those two and we get 24 and 24 turned out to be the total number of letters Okay, so this is the Hoffman tree, and this is enough information to encode and decode every letter. Okay, how do we obtain the code words. All the. left. branches. R zero and all the right branches. Are one. Okay, so hey how do I encode a well it's gonna be 11011101 where the code length of four right be is going to be 1100 with a coding for see is going to be 111. D is going to be 001 right, so these head length of three he is 000 just three F is one zero, which is to n G is your one which is two. Okay, so we have we have successfully made a variable length comma free. code, why is it comma free it's because of the tree the tree structure prevents any prefix right, because if you kind of recall what happened before let's say I had another letter he let's say he. If it wasn't prefix free That means that he would be kind of want in one of these interior nodes, meaning that the the code for age itself was a prefix. For another code for like C and for a and for be so the problem is how do I know to stop at H or to keep on going. So without any of the interior nodes being letters, we know that, once we get to a leaf right once we get to the bottom we're done and we can just move on, so we know exactly when to stop. Okay, furthermore, this turns out to be a very the most optimal that you can get with a variable length. Fixed no variable length encoding. Okay, how awesome is it how good does it do well oops when I do here. Oh here's a here's a challenge, use the hump and tree above to decrypt this thing and write your answer in the chat. bags right and at first glance you're like how did you know when one letter stopped in one letter started there variable length. But if you just run through it, you can see right that 1100 now i've gotten to a leaf, so this one stops there and that's a B 1101 i've gotten to a leaf so that's a 01 that's G right 01 that's a G 000 that's a E and 001 that's a tea bag. Okay, so just for an example. Alright, so um let's let's compare huffman code versus fixed length code if I used a fixed length code or let's do the huffman code first under huffman encoding the output length is going to be 64 how did you get that well. This is the. code length. Of a a times the frequency. Of a and you do that for each Okay, for example, this guy is the code length. You do that you get 64 That means that this message here take 64 bits. Okay, how how many bits do you need for the. Fixed length encoded. So for the fixed length encoding since there are 123456 since there's seven bits. says there are seven bits I need three bits per character right. So, since there are seven bits we need three bits per character, so the file length will be well three times 24 because there's 24 letters each letter is the same, and you get 72 So you can see, of what a difference, it makes to use this huffman and coding. Okay, any questions about these computations these calculations. um so in the last few minutes, I want to share with you another variable length and coding, which I think is really cool. and actually does have some practical use. But I don't think it's super common. Okay, so suppose we want to store a sequence of positive integers in a particular range let's say one to 100, for example, you can have to 524 15 whatever right so um. If I have a general idea that here's kind of a general thing is you. generally. expect. shorter or yeah sorry you generally expect more. Smaller. numbers. Then bigger numbers. Which means that you want your smaller numbers to be shorter and your bigger numbers to be longer great all we have to do is just change them into their binary numbers. Because binary expansions of small numbers are short binary experiences of big numbers are long right, for example, two is just 105 is 101 right 24 is bigger and you need five bits so this seems like it's a great way to do it, what is the problem here does anybody know. The comma. Right, like the comma comma is not good, because how do you how do you encode that. coated with bits itself a bunch of ones, to use the ascii code and then even if you do that, since the length of the. of each binary expansion is variable, how do you know when it ends and when the comma begins. Right so there's a lot of problems with this it's probably much better off for you to just Since you know what the limit is it's probably better off of you just. encode everybody, with seven bits do a fixed encoding right and just say two is going to be 1234567 that's going to be two and then five is going to be 1234101. Right it's much better to do it that way 0011000 15 would be 0001111 because then you don't need. Any limiters but you kind of lose this nice thing, where the small numbers are shorter and the big numbers are longer. Okay, so there's this thing called fibonacci encoding and it uses this. This theorem called can endorse theorem that I don't know for. But will state it, and this has a lot to do with the domino timings right, this is actually related. And the idea here is that every positive integer can be uniquely written as the sum of nonconsecutive fibonacci numbers so, for example, 99 is equal to 89 plus eight plus to notice that they're not consecutive right. 4734 plus 13 nonconsecutive right. 375 is you know this whole thing and notice that they're not consecutive to 3389 3435 and right there there's a space between them. Okay, so. um. This reminds us, maybe of. binary expansions right if I if I take the binary numbers and the fibonacci numbers right it's you basically put a one in the column, if you have if that's one of your one of the terms in your expansion so, for example, 99 in fibonacci is going to be. We, what do we do for nine we got to an eight and a 89, so this is going to be the expansion, whereas for. For binary it would be. One to 32 and 64 so the binary expansion of course is shorter that's the theoretically optimal expansion. But. The expansion of the fibonacci numbers has a very special property. Okay. there's only one way to write each positive integers is some of non consecutive fibonacci numbers if if the fibonacci numbers are not consecutive That means that the binary string avoids what little sub string. Exactly it avoids the sub string one one. So every expansion avoids that string so you can use that string as a comma. And the smaller numbers are shorter and the bigger numbers are longer. Okay, so let's do an example here. What we're gonna do is we're going to expand each number two five. Right we're going to expand each number using its fibonacci expansion loops. You kind of write them backwards. And so, since you've written them backwards, they all end in a one right notice that they all end in one. One is just one right, so the code is going to be expand the number using this fibonacci expansion, then write a single one for the comma then write the next number, and then a single one the next number single one, and so on. So it's a pretty cool way to do it, it makes it so that you do get this variable length encoding of all the integers, so this is sort of like you know you could you could. You don't need to use like the Hoffman tree or anything like that. So. You need around one plus log base 1.6 to have an bits this comes from the. The fi right one plus the square to five over to to encode an integer using fibonacci encoding. And for each integer in general, this is not as efficient as based to but remember the problem that we found with base to is that if you use that, then you need commas and commas are bad, so this way each number is a little bit longer but you're able to do it without commerce. Right, so the fibonacci encoding uses bits to encode this particular example that I showed you. use a binary and eight bit ascii comma, how many bits would that take well. How many colors do you need. You need. 1234567899 commas. So just the commas. Take nine times eight which is 72 bits. Just the cause, but we're able to do it in 54 bits so that's that's a much, much better. Okay, so next time we're going to find a lower bound on the number of bits required to encode certain math objects and the end the theoretical optimal encoding. is not necessarily going to be a variable length and coding or a or a fixed length encoding it's much more like what some of the students were saying in the beginning of class where. You you take bigger chunks if they're if they're strings you kind of take bigger chunks and crunch them down as far as you possibly can, and we'll find out what that limit is. Okay, any questions or comments you want me to go back to anything, or I think that's kind of it for the class today. and i'll stick around for another 10 minutes or so. Okay, I guess that's it then so see you all on Thursday.